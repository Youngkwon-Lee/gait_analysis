"""
HPC í›ˆë ¨ ê²°ê³¼ ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸

ROC Curve, Confusion Matrix, ì„±ëŠ¥ ë¹„êµí‘œ ìƒì„±
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from sklearn.metrics import roc_curve, auc, confusion_matrix
import json

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
plt.rcParams['axes.unicode_minus'] = False

# ê²½ë¡œ ì„¤ì •
BASE_PATH = Path(__file__).parent.parent
RESULTS_PATH = BASE_PATH / 'results'
VISUALIZATIONS_PATH = BASE_PATH / 'visualizations'
VISUALIZATIONS_PATH.mkdir(exist_ok=True)

# ë² ì´ìŠ¤ë¼ì¸ ë…¼ë¬¸ ê²°ê³¼ (arXiv:2503.05708)
BASELINE_RESULTS = {
    'PD_Screening': {'auc': 0.821, 'balanced_acc': 0.639},
    'OA_Screening': {'auc': 0.990, 'balanced_acc': 0.942},
    'CVA_Detection': {'auc': 0.950, 'balanced_acc': 0.747},
    'PD_vs_CVA': {'auc': 0.657, 'balanced_acc': 0.607}
}

# Task ì´ë¦„ ë§¤í•‘
TASK_NAMES = {
    'PD_Screening': 'PD Screening (HS vs PD)',
    'OA_Screening': 'OA Screening (HS vs HOA)',
    'CVA_Detection': 'CVA Detection (HS vs CVA)',
    'PD_vs_CVA': 'PD vs CVA'
}


def load_results():
    """HPC í›ˆë ¨ ê²°ê³¼ CSV íŒŒì¼ ë¡œë“œ"""
    results = {}

    csv_files = sorted(RESULTS_PATH.glob('dl_baseline_results_*.csv'))

    if not csv_files:
        print("âŒ ê²°ê³¼ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ê²½ë¡œ: {RESULTS_PATH}")
        print("\nHPCì—ì„œ ê²°ê³¼ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”:")
        print("bash scripts/download_hpc_results.sh")
        return None

    print(f"âœ… {len(csv_files)}ê°œ ê²°ê³¼ íŒŒì¼ ë°œê²¬")

    for csv_file in csv_files:
        df = pd.read_csv(csv_file)

        # Task ì´ë¦„ ì¶”ì¶œ
        task_name = df['task'].iloc[0]
        results[task_name] = df

        print(f"  - {task_name}: {csv_file.name}")

    return results


def plot_roc_curves(results):
    """ROC Curve ì‹œê°í™” (4ê°œ Task í•œ ê·¸ë˜í”„ì—)"""

    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    axes = axes.flatten()

    for idx, (task_name, df) in enumerate(results.items()):
        ax = axes[idx]

        # ROC ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if 'fpr' in df.columns and 'tpr' in df.columns:
            # JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥ëœ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
            fpr = json.loads(df['fpr'].iloc[0])
            tpr = json.loads(df['tpr'].iloc[0])
            roc_auc = df['roc_auc'].iloc[0]

            # ROC Curve ê·¸ë¦¬ê¸°
            ax.plot(fpr, tpr, color='darkorange', lw=2,
                   label=f'ìš°ë¦¬ ëª¨ë¸ (AUC = {roc_auc:.3f})')

            # ë² ì´ìŠ¤ë¼ì¸ ë¹„êµì„  (AUCë§Œ í‘œì‹œ)
            baseline_auc = BASELINE_RESULTS[task_name]['auc']
            ax.axhline(y=baseline_auc, color='blue', linestyle='--', lw=1.5,
                      label=f'ë…¼ë¬¸ (AUC = {baseline_auc:.3f})')
        else:
            # ROC ë°ì´í„° ì—†ìœ¼ë©´ ë¹ˆ ê·¸ë˜í”„
            ax.text(0.5, 0.5, 'ROC ë°ì´í„° ì—†ìŒ', ha='center', va='center')

        # ëŒ€ê°ì„  (ëœë¤ ë¶„ë¥˜ê¸°)
        ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.5)

        # ê·¸ë˜í”„ ì„¤ì •
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate', fontsize=11)
        ax.set_ylabel('True Positive Rate', fontsize=11)
        ax.set_title(f'{TASK_NAMES[task_name]}', fontsize=13, fontweight='bold')
        ax.legend(loc="lower right", fontsize=9)
        ax.grid(True, alpha=0.3)

    plt.tight_layout()

    # ì €ì¥
    save_path = VISUALIZATIONS_PATH / 'roc_curves_all_tasks.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… ROC Curves ì €ì¥: {save_path}")

    plt.close()


def plot_confusion_matrices(results):
    """Confusion Matrix ì‹œê°í™” (4ê°œ Task)"""

    fig, axes = plt.subplots(2, 2, figsize=(14, 12))
    axes = axes.flatten()

    for idx, (task_name, df) in enumerate(results.items()):
        ax = axes[idx]

        # Confusion Matrix ë°ì´í„° í™•ì¸
        if 'tn' in df.columns and 'fp' in df.columns:
            tn = df['tn'].iloc[0]
            fp = df['fp'].iloc[0]
            fn = df['fn'].iloc[0]
            tp = df['tp'].iloc[0]

            cm = np.array([[tn, fp], [fn, tp]])

            # Heatmap ê·¸ë¦¬ê¸°
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                       cbar_kws={'label': 'Count'},
                       xticklabels=['Class 0', 'Class 1'],
                       yticklabels=['Class 0', 'Class 1'])

            # ì •í™•ë„ ê³„ì‚°
            accuracy = (tn + tp) / (tn + fp + fn + tp)
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0

            # ì œëª©ì— ì„±ëŠ¥ ì§€í‘œ ì¶”ê°€
            title = f'{TASK_NAMES[task_name]}\n'
            title += f'Acc: {accuracy:.3f} | Sens: {sensitivity:.3f} | Spec: {specificity:.3f}'

        else:
            # ë°ì´í„° ì—†ìœ¼ë©´ ë¹ˆ ê·¸ë˜í”„
            ax.text(0.5, 0.5, 'Confusion Matrix ë°ì´í„° ì—†ìŒ',
                   ha='center', va='center')
            title = TASK_NAMES[task_name]

        ax.set_title(title, fontsize=11, fontweight='bold')
        ax.set_xlabel('Predicted Label', fontsize=10)
        ax.set_ylabel('True Label', fontsize=10)

    plt.tight_layout()

    # ì €ì¥
    save_path = VISUALIZATIONS_PATH / 'confusion_matrices_all_tasks.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Confusion Matrices ì €ì¥: {save_path}")

    plt.close()


def plot_performance_comparison(results):
    """ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ì„±ëŠ¥ ë¹„êµ ë§‰ëŒ€ê·¸ë˜í”„"""

    # ë°ì´í„° ì¤€ë¹„
    tasks = []
    our_auc = []
    baseline_auc = []
    our_acc = []
    baseline_acc = []

    for task_name, df in results.items():
        tasks.append(TASK_NAMES[task_name])
        our_auc.append(df['roc_auc'].iloc[0])
        baseline_auc.append(BASELINE_RESULTS[task_name]['auc'])
        our_acc.append(df['balanced_accuracy'].iloc[0])
        baseline_acc.append(BASELINE_RESULTS[task_name]['balanced_acc'])

    # 2ê°œ ì„œë¸Œí”Œë¡¯ (AUC, Balanced Accuracy)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    x = np.arange(len(tasks))
    width = 0.35

    # AUC ë¹„êµ
    bars1 = ax1.bar(x - width/2, our_auc, width, label='ìš°ë¦¬ ê²°ê³¼', color='#FF6B6B')
    bars2 = ax1.bar(x + width/2, baseline_auc, width, label='ë…¼ë¬¸ (Baseline)', color='#4ECDC4')

    ax1.set_xlabel('Task', fontsize=12, fontweight='bold')
    ax1.set_ylabel('ROC-AUC', fontsize=12, fontweight='bold')
    ax1.set_title('ROC-AUC ë¹„êµ', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(tasks, rotation=15, ha='right')
    ax1.legend(fontsize=11)
    ax1.set_ylim([0.5, 1.0])
    ax1.grid(True, alpha=0.3, axis='y')

    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}',
                    ha='center', va='bottom', fontsize=9)

    # Balanced Accuracy ë¹„êµ
    bars3 = ax2.bar(x - width/2, our_acc, width, label='ìš°ë¦¬ ê²°ê³¼', color='#FF6B6B')
    bars4 = ax2.bar(x + width/2, baseline_acc, width, label='ë…¼ë¬¸ (Baseline)', color='#4ECDC4')

    ax2.set_xlabel('Task', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Balanced Accuracy', fontsize=12, fontweight='bold')
    ax2.set_title('Balanced Accuracy ë¹„êµ', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(tasks, rotation=15, ha='right')
    ax2.legend(fontsize=11)
    ax2.set_ylim([0.5, 1.0])
    ax2.grid(True, alpha=0.3, axis='y')

    # ë§‰ëŒ€ ìœ„ì— ê°’ í‘œì‹œ
    for bars in [bars3, bars4]:
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}',
                    ha='center', va='bottom', fontsize=9)

    plt.tight_layout()

    # ì €ì¥
    save_path = VISUALIZATIONS_PATH / 'performance_comparison.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„ ì €ì¥: {save_path}")

    plt.close()


def create_performance_table(results):
    """ì„±ëŠ¥ ì§€í‘œ ìƒì„¸ ë¹„êµí‘œ ìƒì„± (ë§ˆí¬ë‹¤ìš´)"""

    table = []
    table.append("# ì„±ëŠ¥ ì§€í‘œ ìƒì„¸ ë¹„êµ")
    table.append("")
    table.append("| Task | Metric | ìš°ë¦¬ ê²°ê³¼ | ë…¼ë¬¸ (Baseline) | ê°œì„ ë„ | ë“±ê¸‰ |")
    table.append("|------|--------|-----------|-----------------|--------|------|")

    for task_name, df in results.items():
        our_auc = df['roc_auc'].iloc[0]
        our_acc = df['balanced_accuracy'].iloc[0]
        baseline_auc = BASELINE_RESULTS[task_name]['auc']
        baseline_acc = BASELINE_RESULTS[task_name]['balanced_acc']

        auc_diff = our_auc - baseline_auc
        acc_diff = our_acc - baseline_acc

        # ê°œì„ ë„ì— ë”°ë¥¸ ë“±ê¸‰
        auc_grade = "ğŸ”¥ğŸ”¥ğŸ”¥" if auc_diff > 0.2 else "ğŸ”¥ğŸ”¥" if auc_diff > 0.1 else "âœ…" if auc_diff > 0 else "âš ï¸"
        acc_grade = "ğŸ”¥ğŸ”¥ğŸ”¥" if acc_diff > 0.2 else "ğŸ”¥ğŸ”¥" if acc_diff > 0.1 else "âœ…" if acc_diff > 0 else "âš ï¸"

        # AUC í–‰
        table.append(f"| {TASK_NAMES[task_name]} | **ROC-AUC** | **{our_auc:.3f}** | {baseline_auc:.3f} | **{auc_diff:+.1%}** | {auc_grade} |")

        # Balanced Accuracy í–‰
        table.append(f"| {TASK_NAMES[task_name]} | **Balanced Acc** | **{our_acc:.3f}** | {baseline_acc:.3f} | **{acc_diff:+.1%}** | {acc_grade} |")

        # Sensitivity/Specificity í–‰ (ìš°ë¦¬ ê²°ê³¼ë§Œ)
        if 'sensitivity' in df.columns:
            sensitivity = df['sensitivity'].iloc[0]
            specificity = df['specificity'].iloc[0]
            table.append(f"| {TASK_NAMES[task_name]} | Sensitivity | {sensitivity:.3f} | - | - | - |")
            table.append(f"| {TASK_NAMES[task_name]} | Specificity | {specificity:.3f} | - | - | - |")

    table.append("")
    table.append("## ê°œì„ ë„ ë“±ê¸‰")
    table.append("- ğŸ”¥ğŸ”¥ğŸ”¥: >20% ê°œì„ ")
    table.append("- ğŸ”¥ğŸ”¥: 10-20% ê°œì„ ")
    table.append("- âœ…: 0-10% ê°œì„ ")
    table.append("- âš ï¸: ë…¼ë¬¸ë³´ë‹¤ ë‚®ìŒ")

    # ì €ì¥
    save_path = VISUALIZATIONS_PATH / 'PERFORMANCE_COMPARISON_TABLE.md'
    save_path.write_text('\n'.join(table), encoding='utf-8')
    print(f"âœ… ì„±ëŠ¥ ë¹„êµí‘œ ì €ì¥: {save_path}")

    # í„°ë¯¸ë„ ì¶œë ¥
    print("\n" + '\n'.join(table))


def main():
    print("=" * 60)
    print("HPC í›ˆë ¨ ê²°ê³¼ ì‹œê°í™” ìŠ¤í¬ë¦½íŠ¸")
    print("=" * 60)

    # ê²°ê³¼ íŒŒì¼ ë¡œë“œ
    results = load_results()

    if results is None:
        return

    print("\n" + "=" * 60)
    print("ì‹œê°í™” ìƒì„± ì¤‘...")
    print("=" * 60)

    # 1. ROC Curves
    print("\n1. ROC Curves ìƒì„± ì¤‘...")
    plot_roc_curves(results)

    # 2. Confusion Matrices
    print("\n2. Confusion Matrices ìƒì„± ì¤‘...")
    plot_confusion_matrices(results)

    # 3. Performance Comparison
    print("\n3. ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    plot_performance_comparison(results)

    # 4. Performance Table
    print("\n4. ì„±ëŠ¥ ë¹„êµí‘œ ìƒì„± ì¤‘...")
    create_performance_table(results)

    print("\n" + "=" * 60)
    print(f"âœ… ëª¨ë“  ì‹œê°í™” ì™„ë£Œ!")
    print(f"ì €ì¥ ê²½ë¡œ: {VISUALIZATIONS_PATH}")
    print("=" * 60)


if __name__ == '__main__':
    main()
